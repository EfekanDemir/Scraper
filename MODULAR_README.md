# ğŸ—ï¸ **ModÃ¼ler Web Scraper - DokÃ¼mantasyon**

## ğŸ“‹ **Proje YapÄ±sÄ±**

```
Scraper/
â”œâ”€â”€ main_scraper.py              # Ana scraper dosyasÄ±
â”œâ”€â”€ modules/                     # YardÄ±mcÄ± modÃ¼ller
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ web_client.py           # Web istekleri
â”‚   â”œâ”€â”€ html_parser.py          # HTML parsing
â”‚   â”œâ”€â”€ js_extractor.py         # JavaScript veri Ã§Ä±karma
â”‚   â”œâ”€â”€ api_client.py           # API Ã§aÄŸrÄ±larÄ±
â”‚   â””â”€â”€ data_exporter.py        # Veri dÄ±ÅŸa aktarma
â”œâ”€â”€ requirements.txt            # Gerekli kÃ¼tÃ¼phaneler
â””â”€â”€ MODULAR_README.md          # Bu dosya
```

## ğŸ¯ **ModÃ¼ler YapÄ±nÄ±n AvantajlarÄ±**

### âœ… **Sorumluluk AyrÄ±mÄ±**
- Her modÃ¼l tek bir iÅŸlevi yerine getirir
- Kod tekrarÄ± Ã¶nlenir
- BakÄ±m ve gÃ¼ncelleme kolaylaÅŸÄ±r

### âœ… **Yeniden KullanÄ±labilirlik**
- ModÃ¼ller baÄŸÄ±msÄ±z olarak kullanÄ±labilir
- FarklÄ± projelerde tekrar kullanÄ±labilir
- Test edilebilirlik artar

### âœ… **GeniÅŸletilebilirlik**
- Yeni modÃ¼ller kolayca eklenebilir
- Mevcut modÃ¼ller gÃ¼ncellenebilir
- Yeni Ã¶zellikler modÃ¼ler ÅŸekilde eklenebilir

## ğŸ”§ **ModÃ¼l DetaylarÄ±**

### 1. **web_client.py** - Web Ä°stekleri
```python
from modules.web_client import WebClient

# WebClient Ã¶rneÄŸi oluÅŸtur
client = WebClient(
    user_agent="Custom User Agent",
    timeout=30,
    use_selenium=False,
    rate_limit=1.0
)

# HTML iÃ§eriÄŸi al
soup = client.get_soup("https://example.com")
```

**Ã–zellikler:**
- Requests ve Selenium desteÄŸi
- Rate limiting
- Hata yÃ¶netimi
- Otomatik kaynak temizleme

### 2. **html_parser.py** - HTML Parsing
```python
from modules.html_parser import HTMLParser

# HTMLParser Ã¶rneÄŸi oluÅŸtur
parser = HTMLParser()

# Veri Ã§ek
scan_info = parser.parse_scan_information(soup)
rank_summary = parser.parse_rank_summary(soup)
competitors = parser.parse_competitors(soup)
```

**Ã–zellikler:**
- Scan Information parsing
- Rank Summary parsing
- Competitors parsing
- Sponsorlu listeler parsing
- DetaylÄ± sonuÃ§lar parsing

### 3. **js_extractor.py** - JavaScript Veri Ã‡Ä±karma
```python
from modules.js_extractor import JSExtractor

# JSExtractor Ã¶rneÄŸi oluÅŸtur
extractor = JSExtractor()

# JavaScript verilerini Ã§Ä±kar
js_data = extractor.extract_all_js_data(soup)
map_data = extractor.extract_map_data(js_data)
```

**Ã–zellikler:**
- var pinz array Ã§Ä±karma
- scan_guid Ã§Ä±karma
- place_id Ã§Ä±karma
- Harita verileri Ã§Ä±karma
- GÃ¼venli JSON parsing

### 4. **api_client.py** - API Ã‡aÄŸrÄ±larÄ±
```python
from modules.api_client import APIClient

# APIClient Ã¶rneÄŸi oluÅŸtur
api_client = APIClient(
    user_agent="Custom User Agent",
    timeout=30,
    rate_limit=1.0
)

# API verilerini Ã§ek
api_data = api_client.get_all_api_data(base_url, js_data)
```

**Ã–zellikler:**
- Competitors API Ã§aÄŸrÄ±larÄ±
- Analytics API Ã§aÄŸrÄ±larÄ±
- Rate limiting
- Hata yÃ¶netimi
- HTML/JSON response parsing

### 5. **data_exporter.py** - Veri DÄ±ÅŸa Aktarma
```python
from modules.data_exporter import DataExporter

# DataExporter Ã¶rneÄŸi oluÅŸtur
exporter = DataExporter()

# Verileri dÄ±ÅŸa aktar
exporter.save_to_json(data, "output.json")
exporter.save_to_excel(data, "output.xlsx")
exporter.save_to_csv(data, "output")
```

**Ã–zellikler:**
- JSON formatÄ±nda kaydetme
- Excel formatÄ±nda kaydetme (Ã§ok sayfalÄ±)
- CSV formatÄ±nda kaydetme
- Otomatik sÃ¼tun geniÅŸliÄŸi ayarlama
- Zaman damgalÄ± dosya adlarÄ±

## ğŸš€ **KullanÄ±m Ã–rnekleri**

### **Basit KullanÄ±m**
```python
from main_scraper import MainScraper

# Scraper oluÅŸtur
scraper = MainScraper(
    use_selenium=False,
    rate_limit=1.0,
    timeout=30
)

# Tam iÅŸlemi Ã§alÄ±ÅŸtÄ±r
success = scraper.run("https://example.com", "output_data")
```

### **GeliÅŸmiÅŸ KullanÄ±m**
```python
from main_scraper import MainScraper

# Ã–zel ayarlarla scraper oluÅŸtur
scraper = MainScraper(
    user_agent="Custom Bot 1.0",
    use_selenium=True,
    headless=True,
    rate_limit=2.0,
    timeout=60
)

# Sadece veri Ã§ek
data = scraper.scrape_all("https://example.com")

# Manuel olarak dÄ±ÅŸa aktar
scraper.export_data(data, "custom_output")
```

### **ModÃ¼l BazlÄ± KullanÄ±m**
```python
# Sadece HTML parsing
from modules.html_parser import HTMLParser
from modules.web_client import WebClient

client = WebClient()
parser = HTMLParser()

soup = client.get_soup("https://example.com")
scan_info = parser.parse_scan_information(soup)
```

## ğŸ“Š **Ã‡Ä±ktÄ± FormatlarÄ±**

### **JSON FormatÄ±**
```json
{
  "ozet_bilgiler": {
    "Ä°ÅŸletme AdÄ±": "Kanal-Immobilien GmbH",
    "Adres": "TorstraÃŸe 18",
    "Ranked Locations": "1/49"
  },
  "rakipler": [...],
  "sponsorlu_listeler": [...],
  "metadata": {
    "scraped_at": "2025-08-07T19:31:35",
    "scraper_version": "4.0",
    "method": "modular_hybrid"
  }
}
```

### **Excel FormatÄ±**
- **Ã–zet Bilgiler** sayfasÄ±
- **Rakipler** sayfasÄ±
- **Sponsorlu Listeler** sayfasÄ±
- **DetaylÄ± SonuÃ§lar** sayfasÄ±
- **Harita Verileri** sayfasÄ±

### **CSV FormatÄ±**
- Her veri tÃ¼rÃ¼ iÃ§in ayrÄ± CSV dosyasÄ±
- UTF-8 encoding
- VirgÃ¼lle ayrÄ±lmÄ±ÅŸ deÄŸerler

## âš™ï¸ **KonfigÃ¼rasyon**

### **Rate Limiting**
```python
# HÄ±zlÄ± scraping (dikkatli kullanÄ±n)
scraper = MainScraper(rate_limit=0.5)

# YavaÅŸ scraping (gÃ¼venli)
scraper = MainScraper(rate_limit=2.0)
```

### **Selenium KullanÄ±mÄ±**
```python
# Selenium ile (dinamik iÃ§erik iÃ§in)
scraper = MainScraper(use_selenium=True, headless=True)

# Requests ile (statik iÃ§erik iÃ§in)
scraper = MainScraper(use_selenium=False)
```

### **Timeout AyarlarÄ±**
```python
# KÄ±sa timeout
scraper = MainScraper(timeout=10)

# Uzun timeout (yavaÅŸ siteler iÃ§in)
scraper = MainScraper(timeout=60)
```

## ğŸ› ï¸ **Hata YÃ¶netimi**

### **Genel Hatalar**
```python
try:
    success = scraper.run(url, "output")
except Exception as e:
    print(f"Hata: {e}")
finally:
    scraper.cleanup()
```

### **ModÃ¼l BazlÄ± Hata YÃ¶netimi**
```python
# WebClient hatalarÄ±
soup = client.get_soup(url)
if not soup:
    print("HTML iÃ§eriÄŸi alÄ±namadÄ±")

# API hatalarÄ±
api_data = api_client.get_all_api_data(base_url, js_data)
if not api_data:
    print("API verileri alÄ±namadÄ±")
```

## ğŸ“ˆ **Performans Optimizasyonu**

### **Bellek KullanÄ±mÄ±**
- Her modÃ¼l baÄŸÄ±msÄ±z Ã§alÄ±ÅŸÄ±r
- Gereksiz veri saklanmaz
- Otomatik kaynak temizleme

### **HÄ±z Optimizasyonu**
- Paralel iÅŸlemler mÃ¼mkÃ¼n
- Rate limiting ile sunucu yÃ¼kÃ¼ azaltÄ±lÄ±r
- Selenium sadece gerektiÄŸinde kullanÄ±lÄ±r

### **Veri DoÄŸruluÄŸu**
- Ã‡oklu doÄŸrulama yÃ¶ntemleri
- Hata durumunda varsayÄ±lan deÄŸerler
- DetaylÄ± loglama

## ğŸ”„ **GeliÅŸtirme ve GeniÅŸletme**

### **Yeni ModÃ¼l Ekleme**
```python
# modules/new_module.py
class NewModule:
    def __init__(self):
        pass
    
    def process_data(self, data):
        # Ä°ÅŸlem mantÄ±ÄŸÄ±
        return processed_data
```

### **Ana Scraper'a Entegrasyon**
```python
# main_scraper.py
from modules.new_module import NewModule

class MainScraper:
    def __init__(self):
        self.new_module = NewModule()
    
    def scrape_all(self, url):
        # ... mevcut kod ...
        results["new_data"] = self.new_module.process_data(data)
        return results
```

## ğŸ‰ **SonuÃ§**

ModÃ¼ler yapÄ± sayesinde:
- âœ… Kod daha organize ve anlaÅŸÄ±lÄ±r
- âœ… BakÄ±m ve gÃ¼ncelleme kolaylaÅŸÄ±r
- âœ… Yeniden kullanÄ±labilirlik artar
- âœ… Test edilebilirlik geliÅŸir
- âœ… GeniÅŸletilebilirlik saÄŸlanÄ±r

Bu yapÄ±, bÃ¼yÃ¼k Ã¶lÃ§ekli scraping projeleri iÃ§in ideal bir temel oluÅŸturur.


