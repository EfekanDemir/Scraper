#!/usr/bin/env python3
"""
Main Scraper - ModÃ¼ler Web Scraping AracÄ±
Local Rank Report sitesinden veri Ã§ekme aracÄ±

Bu dosya tÃ¼m yardÄ±mcÄ± modÃ¼lleri kullanarak hibrit scraping yapar:
- modules/html_parser.py: HTML parsing iÅŸlemleri
- modules/js_extractor.py: JavaScript veri Ã§Ä±karma
- modules/api_client.py: API Ã§aÄŸrÄ±larÄ±
- modules/web_client.py: Web istekleri
- modules/data_exporter.py: Veri dÄ±ÅŸa aktarma
"""

import sys
import os
from typing import Dict, Any, Optional
from datetime import datetime
from urllib.parse import urlparse

# Modules klasÃ¶rÃ¼nÃ¼ Python path'ine ekle
sys.path.append(os.path.join(os.path.dirname(__file__), 'modules'))

from web_client import WebClient
from html_parser import HTMLParser
from js_extractor import JSExtractor
from api_client import APIClient
from data_exporter import DataExporter


class MainScraper:
    """Ana scraper sÄ±nÄ±fÄ± - tÃ¼m modÃ¼lleri koordine eder"""
    
    def __init__(self, 
                 user_agent: str = None,
                 timeout: int = 30,
                 use_selenium: bool = False,
                 headless: bool = True,
                 rate_limit: float = 1.0):
        """
        MainScraper sÄ±nÄ±fÄ±nÄ± baÅŸlatÄ±r.
        
        Args:
            user_agent: User-Agent string'i
            timeout: Ä°stek zaman aÅŸÄ±mÄ±
            use_selenium: Selenium kullanÄ±mÄ±
            headless: Selenium headless modu
            rate_limit: Rate limiting sÃ¼resi
        """
        # ModÃ¼lleri baÅŸlat
        self.web_client = WebClient(
            user_agent=user_agent,
            timeout=timeout,
            use_selenium=use_selenium,
            headless=headless,
            rate_limit=rate_limit
        )
        
        self.html_parser = HTMLParser()
        self.js_extractor = JSExtractor()
        self.api_client = APIClient(
            user_agent=user_agent,
            timeout=timeout,
            rate_limit=rate_limit
        )
        self.data_exporter = DataExporter()
        
        # AyarlarÄ± sakla
        self.use_selenium = use_selenium
    
    def scrape_all(self, url: str) -> Dict[str, Any]:
        """
        TÃ¼m verileri hibrit yÃ¶ntemle Ã§eker.
        
        Args:
            url: Hedef URL
            
        Returns:
            TÃ¼m Ã§ekilen veriler
        """
        print(f"Veri Ã§ekme iÅŸlemi baÅŸlatÄ±lÄ±yor: {url}")
        
        # 1. HTML iÃ§eriÄŸini al
        print("1. HTML iÃ§eriÄŸi alÄ±nÄ±yor...")
        soup = self.web_client.get_soup(url)
        if not soup:
            print("âŒ HTML iÃ§eriÄŸi alÄ±namadÄ±")
            return {}
        print("âœ… HTML iÃ§eriÄŸi baÅŸarÄ±yla alÄ±ndÄ±")
        
        # 2. JavaScript verilerini Ã§Ä±kar
        print("2. JavaScript verileri Ã§Ä±karÄ±lÄ±yor...")
        js_data = self.js_extractor.extract_all_js_data(soup)
        print(f"âœ… JavaScript verileri Ã§Ä±karÄ±ldÄ±: {len(js_data)} alan")
        
        # 3. HTML parsing ile temel verileri Ã§ek
        results = {}
        
        print("3. Ã–zet bilgiler Ã§ekiliyor...")
        scan_info = self.html_parser.parse_scan_information(soup)
        rank_summary = self.html_parser.parse_rank_summary(soup)
        results["ozet_bilgiler"] = {**scan_info, **rank_summary}
        
        print("4. Rakipler Ã§ekiliyor...")
        results["rakipler"] = self.html_parser.parse_competitors(soup)
        
        print("5. Sponsorlu listeler Ã§ekiliyor...")
        results["sponsorlu_listeler"] = self.html_parser.parse_sponsorlu_listeler(soup)
        
        print("6. DetaylÄ± sonuÃ§lar Ã§ekiliyor...")
        results["detayli_sonuclar"] = self.html_parser.parse_detayli_sonuclar(soup)
        
        print("7. Harita verileri Ã§ekiliyor...")
        results["harita_verileri"] = self.js_extractor.extract_map_data(js_data)
        
        # 4. API verilerini Ã§ek
        print("8. API verileri Ã§ekiliyor...")
        base_url = f"{urlparse(url).scheme}://{urlparse(url).netloc}"
        results["api_verileri"] = self.api_client.get_all_api_data(base_url, js_data)
        
        # 5. JavaScript verilerini ekle
        results["javascript_verileri"] = js_data
        
        # 6. Metadata ekle
        results["metadata"] = {
            "scraped_at": datetime.now().isoformat(),
            "url": url,
            "scraper_version": "4.0",
            "method": "modular_hybrid",
            "selenium_used": self.use_selenium
        }
        
        return results
    
    def export_data(self, data: Dict[str, Any], base_filename: str = "modular_scraped_data") -> Dict[str, bool]:
        """
        Verileri tÃ¼m formatlarda dÄ±ÅŸa aktarÄ±r.
        
        Args:
            data: Ã‡ekilen veriler
            base_filename: Temel dosya adÄ±
            
        Returns:
            Her format iÃ§in baÅŸarÄ± durumu
        """
        print("\n9. Veriler dÄ±ÅŸa aktarÄ±lÄ±yor...")
        
        # Zaman damgalÄ± dosya adÄ± oluÅŸtur
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{base_filename}_{timestamp}"
        
        # TÃ¼m formatlarda dÄ±ÅŸa aktar
        results = self.data_exporter.export_all_formats(data, filename)
        
        # Ã–zet gÃ¶ster
        self.data_exporter.print_summary(data)
        
        return results
    
    def run(self, url: str, base_filename: str = "modular_scraped_data") -> bool:
        """
        Tam scraping iÅŸlemini Ã§alÄ±ÅŸtÄ±rÄ±r.
        
        Args:
            url: Hedef URL
            base_filename: Temel dosya adÄ±
            
        Returns:
            BaÅŸarÄ± durumu
        """
        try:
            # Veri Ã§ek
            data = self.scrape_all(url)
            
            if not data:
                print("âŒ Veri Ã§ekme iÅŸlemi baÅŸarÄ±sÄ±z oldu")
                return False
            
            # DÄ±ÅŸa aktar
            export_results = self.export_data(data, base_filename)
            
            # SonuÃ§larÄ± gÃ¶ster
            print("\n" + "=" * 60)
            print("MODÃœLER SCRAPER SONUÃ‡LARI")
            print("=" * 60)
            
            success_count = sum(export_results.values())
            total_formats = len(export_results)
            
            print(f"BaÅŸarÄ±lÄ± format sayÄ±sÄ±: {success_count}/{total_formats}")
            
            for format_name, success in export_results.items():
                status = "âœ…" if success else "âŒ"
                print(f"{status} {format_name.upper()}")
            
            print("=" * 60)
            print("ğŸ‰ ModÃ¼ler scraper baÅŸarÄ±yla tamamlandÄ±!")
            
            return success_count > 0
            
        except Exception as e:
            print(f"âŒ Hata: {e}")
            return False
        
        finally:
            # KaynaklarÄ± temizle
            self.web_client.cleanup()
    
    def cleanup(self):
        """KaynaklarÄ± temizler."""
        self.web_client.cleanup()


def main():
    """Ana fonksiyon - Ã¶rnek kullanÄ±m"""
    url = "https://www.local-rank.report/scan/97919fde-e478-4081-983f-7e0065b6b5bb"
    
    # MainScraper Ã¶rneÄŸi oluÅŸtur
    scraper = MainScraper(
        use_selenium=False,  # Selenium kullanmadan baÅŸla
        rate_limit=1.0,      # 1 saniye bekleme
        timeout=30
    )
    
    # Tam iÅŸlemi Ã§alÄ±ÅŸtÄ±r
    success = scraper.run(url, "modular_scraped_data")
    
    if success:
        print("\nğŸ“ OluÅŸturulan dosyalar:")
        print("   - modular_scraped_data_YYYYMMDD_HHMMSS.json")
        print("   - modular_scraped_data_YYYYMMDD_HHMMSS.xlsx")
        print("   - modular_scraped_data_YYYYMMDD_HHMMSS_*.csv")
    else:
        print("\nâŒ Ä°ÅŸlem baÅŸarÄ±sÄ±z oldu")


if __name__ == "__main__":
    main()
