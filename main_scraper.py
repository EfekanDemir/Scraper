#!/usr/bin/env python3
"""
Main Scraper - ModÃ¼ler Web Scraping AracÄ±
Local Rank Report sitesinden veri Ã§ekme aracÄ±

Bu dosya tÃ¼m yardÄ±mcÄ± modÃ¼lleri kullanarak hibrit scraping yapar:
- modules/html_parser.py: HTML parsing iÅŸlemleri
- modules/js_extractor.py: JavaScript veri Ã§Ä±karma
- modules/api_client.py: API Ã§aÄŸrÄ±larÄ±
- modules/web_client.py: Web istekleri
- modules/data_exporter.py: Veri dÄ±ÅŸa aktarma
"""

import sys
import os
from typing import Dict, Any, Optional
from datetime import datetime
from urllib.parse import urlparse

# Modules klasÃ¶rÃ¼nÃ¼ Python path'ine ekle
sys.path.append(os.path.join(os.path.dirname(__file__), 'modules'))

from web_client import WebClient
from html_parser import HTMLParser
from js_extractor import JSExtractor
from api_client import APIClient
from data_exporter import DataExporter
from bs4 import BeautifulSoup
import glob


class MainScraper:
    """Ana scraper sÄ±nÄ±fÄ± - tÃ¼m modÃ¼lleri koordine eder"""
    
    def __init__(self, 
                 user_agent: str = None,
                 timeout: int = 30,
                 use_selenium: bool = False,
                 headless: bool = True,
                 rate_limit: float = 1.0,
                 supabase_url: Optional[str] = None,
                 supabase_key: Optional[str] = None):
        """
        MainScraper sÄ±nÄ±fÄ±nÄ± baÅŸlatÄ±r.
        
        Args:
            user_agent: User-Agent string'i
            timeout: Ä°stek zaman aÅŸÄ±mÄ±
            use_selenium: Selenium kullanÄ±mÄ±
            headless: Selenium headless modu
            rate_limit: Rate limiting sÃ¼resi
            supabase_url: Supabase proje URL'si
            supabase_key: Supabase anon/public API anahtarÄ±
        """
        # ModÃ¼lleri baÅŸlat
        self.web_client = WebClient(
            user_agent=user_agent,
            timeout=timeout,
            use_selenium=use_selenium,
            headless=headless,
            rate_limit=rate_limit
        )
        
        self.html_parser = HTMLParser()
        self.js_extractor = JSExtractor()
        self.api_client = APIClient(
            user_agent=user_agent,
            timeout=timeout,
            rate_limit=rate_limit
        )
        # Supabase kimlik bilgilerini doÄŸrudan DataExporter'a geÃ§
        self.data_exporter = DataExporter(
            supabase_url=supabase_url or os.getenv("SUPABASE_URL"),
            supabase_key=supabase_key or os.getenv("SUPABASE_ANON_KEY"),
        )
        
        # AyarlarÄ± sakla
        self.use_selenium = use_selenium
    
    def _load_local_fallback_soup(self) -> Optional[BeautifulSoup]:
        """Site DosyalarÄ± altÄ±ndaki kaydedilmiÅŸ HTML'den Soup Ã¼retir (varsa)."""
        base_dir = os.path.join(os.path.dirname(__file__), 'Site DosyalarÄ±')
        try:
            candidates = glob.glob(os.path.join(base_dir, '*.html'))
            if not candidates:
                # Alt klasÃ¶rlerde de ara
                candidates = glob.glob(os.path.join(base_dir, '**', '*.html'), recursive=True)
            if candidates:
                # En bÃ¼yÃ¼k dosyayÄ± seÃ§ (tam sayfa olma olasÄ±lÄ±ÄŸÄ± yÃ¼ksek)
                candidates.sort(key=lambda p: os.path.getsize(p), reverse=True)
                with open(candidates[0], 'r', encoding='utf-8', errors='ignore') as f:
                    html = f.read()
                return BeautifulSoup(html, 'html.parser')
        except Exception:
            pass
        return None
    
    def scrape_all(self, url: str) -> Dict[str, Any]:
        """
        TÃ¼m verileri hibrit yÃ¶ntemle Ã§eker.
        
        Args:
            url: Hedef URL
            
        Returns:
            TÃ¼m Ã§ekilen veriler
        """
        print(f"Veri Ã§ekme iÅŸlemi baÅŸlatÄ±lÄ±yor: {url}")
        
        # 1. HTML iÃ§eriÄŸini al
        print("1. HTML iÃ§eriÄŸi alÄ±nÄ±yor...")
        soup = self.web_client.get_soup(url)
        # EÄŸer eksik iÃ§erik geldiyse, local fallback dene
        if soup and not soup.find("h4", string=lambda s: s and "Scan Information" in s):
            local_soup = self._load_local_fallback_soup()
            if local_soup:
                print("â„¹ï¸ Uzak sayfa sÄ±nÄ±rlÄ± iÃ§erik dÃ¶ndÃ¼; yerel HTML fallback kullanÄ±lacak")
                soup = local_soup
        if not soup:
            print("âŒ HTML iÃ§eriÄŸi alÄ±namadÄ±")
            return {}
        print("âœ… HTML iÃ§eriÄŸi baÅŸarÄ±yla alÄ±ndÄ±")
        
        # 2. JavaScript verilerini Ã§Ä±kar
        print("2. JavaScript verileri Ã§Ä±karÄ±lÄ±yor...")
        js_data = self.js_extractor.extract_all_js_data(soup)
        print(f"âœ… JavaScript verileri Ã§Ä±karÄ±ldÄ±: {len(js_data)} alan")
        
        # 3. HTML parsing ile temel verileri Ã§ek
        results = {}
        
        print("3. Ã–zet bilgiler Ã§ekiliyor...")
        scan_info = self.html_parser.parse_scan_information(soup)
        rank_summary = self.html_parser.parse_rank_summary(soup)
        results["ozet_bilgiler"] = {**scan_info, **rank_summary}
        
        print("4. Rakipler Ã§ekiliyor...")
        results["rakipler"] = self.html_parser.parse_competitors(soup)
        
        print("5. Sponsorlu listeler Ã§ekiliyor...")
        results["sponsorlu_listeler"] = self.html_parser.parse_sponsorlu_listeler(soup)
        
        print("6. DetaylÄ± sonuÃ§lar Ã§ekiliyor...")
        results["detayli_sonuclar"] = self.html_parser.parse_detayli_sonuclar(soup)
        
        print("7. Harita verileri Ã§ekiliyor...")
        map_data = self.js_extractor.extract_map_data(js_data)
        # JS ile gelmezse analytics fallback ile Ã¼ret
        if not map_data:
            base_url = f"{urlparse(url).scheme}://{urlparse(url).netloc}"
            try:
                fallback_points = self.api_client.get_map_points_from_page(
                    base_url,
                    soup,
                    default_pid=js_data.get("place_id") or None,
                )
                # normalize to similar structure as pins
                map_data = [
                    {
                        "lat": p.get("lat"),
                        "lon": p.get("lon"),
                        "label": None,
                        "title": None,
                        "url": p.get("url"),
                        "color": None,
                        "cid": None,
                        "search_guid": p.get("search_guid"),
                        "pid": p.get("pid"),
                    }
                    for p in fallback_points
                ]
            except Exception as e:
                print(f"Harita fallback hatasÄ±: {e}")
        results["harita_verileri"] = map_data
        
        # 4. API verilerini Ã§ek
        print("8. API verileri Ã§ekiliyor...")
        base_url = f"{urlparse(url).scheme}://{urlparse(url).netloc}"
        results["api_verileri"] = self.api_client.get_all_api_data(base_url, js_data)
        
        # 5. JavaScript verilerini ekle
        results["javascript_verileri"] = js_data
        
        # 6. Metadata ekle
        results["metadata"] = {
            "scraped_at": datetime.now().isoformat(),
            "url": url,
            "scraper_version": "4.0",
            "method": "modular_hybrid",
            "selenium_used": self.use_selenium
        }
        
        return results
    
    def export_data(self, data: Dict[str, Any], base_filename: str = "modular_scraped_data") -> Dict[str, bool]:
        """
        Verileri Supabase'e dÄ±ÅŸa aktarÄ±r.
        
        Args:
            data: Ã‡ekilen veriler
            base_filename: (artÄ±k dosya Ã¼retilmiyor) uyumluluk iÃ§in tutuldu
            
        Returns:
            Her format iÃ§in baÅŸarÄ± durumu
        """
        print("\n9. Veriler Supabase'e aktarÄ±lÄ±yor...")
        
        # TÃ¼m formatlarda dÄ±ÅŸa aktar (yalnÄ±zca Supabase)
        results = self.data_exporter.export_all_formats(data, base_filename)
        
        # Ã–zet gÃ¶ster
        self.data_exporter.print_summary(data)
        
        return results
    
    def run(self, url: str, base_filename: str = "modular_scraped_data") -> bool:
        """
        Tam scraping iÅŸlemini Ã§alÄ±ÅŸtÄ±rÄ±r.
        
        Args:
            url: Hedef URL
            base_filename: Temel dosya adÄ±
            
        Returns:
            BaÅŸarÄ± durumu
        """
        try:
            # Veri Ã§ek
            data = self.scrape_all(url)
            
            if not data:
                print("âŒ Veri Ã§ekme iÅŸlemi baÅŸarÄ±sÄ±z oldu")
                return False
            
            # DÄ±ÅŸa aktar
            export_results = self.export_data(data, base_filename)
            
            # SonuÃ§larÄ± gÃ¶ster
            print("\n" + "=" * 60)
            print("MODÃœLER SCRAPER SONUÃ‡LARI")
            print("=" * 60)
            
            success_count = sum(1 for v in export_results.values() if v)
            total_formats = len(export_results)
            
            print(f"BaÅŸarÄ±lÄ± Ã§Ä±ktÄ± sayÄ±sÄ±: {success_count}/{total_formats}")
            
            for format_name, success in export_results.items():
                status = "âœ…" if success else "âŒ"
                print(f"{status} {format_name.upper()}")
            
            print("=" * 60)
            print("ğŸ‰ Supabase aktarÄ±mÄ± tamamlandÄ±!")
            
            return success_count > 0
            
        except Exception as e:
            print(f"âŒ Hata: {e}")
            return False
        
        finally:
            # KaynaklarÄ± temizle
            self.web_client.cleanup()
    
    def cleanup(self):
        """KaynaklarÄ± temizler."""
        self.web_client.cleanup()


def main():
    """Ana fonksiyon - Ã¶rnek kullanÄ±m"""
    url = "https://www.local-rank.report/scan/97919fde-e478-4081-983f-7e0065b6b5bb"
    
    # Supabase bilgileri:
    # NOT: GeliÅŸtirme aÅŸamasÄ±nda RLS sorunlarÄ± yaÅŸÄ±yorsanÄ±z Service Key kullanabilirsiniz
    # URL: https://fmaqwwjilpcgjwzolrvf.supabase.co
    supabase_url = os.getenv("SUPABASE_URL") or "https://fmaqwwjilpcgjwzolrvf.supabase.co"
    supabase_key = os.getenv("SUPABASE_ANON_KEY") or (
        # GELÄ°ÅTÄ°RME Ä°Ã‡Ä°N: Service Key kullanmak iÃ§in aÅŸaÄŸÄ±daki satÄ±rÄ± aÃ§Ä±n
        # os.getenv("SUPABASE_SERVICE_KEY") or
        # "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImZtYXF3d2ppbHBjZ2p3em9scnZmIiwicm9sZSI6ImFub24iLCJpYXQiOjE3MzM2Mzg1MzMsImV4cCI6MjA0OTIxNDUzM30.nMbOBF8S2-8vz41vP0Jvlvl4JHbP6IoCFhYHWbPY2z4"
        "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImZtYXF3d2ppbHBjZ2p3em9scnZmIiwicm9sZSI6ImFub24iLCJpYXQiOjE3MzM2Mzg1MzMsImV4cCI6MjA0OTIxNDUzM30.nMbOBF8S2-8vz41vP0Jvlvl4JHbP6IoCFhYHWbPY2z4"
    )
    
    # MainScraper Ã¶rneÄŸi oluÅŸtur
    scraper = MainScraper(
        use_selenium=False,  # Selenium kullanmadan baÅŸla
        rate_limit=1.0,      # 1 saniye bekleme
        timeout=30,
        supabase_url=supabase_url,
        supabase_key=supabase_key,
    )
    
    # Tam iÅŸlemi Ã§alÄ±ÅŸtÄ±r
    success = scraper.run(url, "modular_scraped_data")
    
    if success:
        print("\nğŸ“¦ Veriler Supabase'e aktarÄ±ldÄ±. Yerel dosya Ã¼retilmedi.")
    else:
        print("\nâŒ Ä°ÅŸlem baÅŸarÄ±sÄ±z oldu")


if __name__ == "__main__":
    main()


